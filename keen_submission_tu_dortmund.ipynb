{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9461c16a",
   "metadata": {},
   "source": [
    "# Keen HACKATHON : AI for automatic process supervision, TU Dortmund\n",
    "\n",
    "### Team Members :\n",
    "* Sourabh Ranade(sourabhranade96@gmail.com)\n",
    "* Prutha Modak (pruthamodak@gmail.com)\n",
    "* Kavyashree Renukachari (kavyashree94v@gmail.com)\n",
    "* Sayali Barve (sayalibarve.040@gmail.com)\n",
    "* Kartik Kadur (karthikkr36@gmail.com)\n",
    "\n",
    "The work done in this notebook is also available as a project in the gitlab repository :\n",
    "\n",
    "[KEEN Hackathon](https://gitlab.com/keenchallenge1/keenchallenge.git)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fc787",
   "metadata": {},
   "source": [
    "#### Packages used in this notebook can be installed from the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf93d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.59.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (8.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.7.1+cpu in /home/jovyan/.local/lib/python3.7/site-packages (1.7.1+cpu)\n",
      "Requirement already satisfied: torchvision==0.8.2+cpu in /home/jovyan/.local/lib/python3.7/site-packages (0.8.2+cpu)\n",
      "Requirement already satisfied: torchaudio===0.7.2 in /home/jovyan/.local/lib/python3.7/site-packages (0.7.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cpu) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cpu) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cpu) (8.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install necessary packages if not installed\n",
    "!pip install tqdm\n",
    "!pip install numpy\n",
    "!pip install Pillow\n",
    "!pip install torch==1.7.1+cpu torchvision==0.8.2+cpu torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a464aff",
   "metadata": {},
   "source": [
    "#### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad28f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "\n",
    "from torch.nn import init\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from random import shuffle\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd23940d",
   "metadata": {},
   "source": [
    "### Set the proper device for execution\n",
    "Set the proper device for moving the tensors and model while training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fc8508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34183c53",
   "metadata": {},
   "source": [
    "### Index your labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afab05b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {\"bad\" : 0, \"good\" : 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73972344",
   "metadata": {},
   "source": [
    "### Dataloader\n",
    "Below cell defines a dataloader for efficient loading of the dataset. In the training mode, the dataloader applies various data augmentation on the input image so that the model tries to generalize th data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9deef931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeenDataloader():\n",
    "    def __init__(self, root, \n",
    "                       labels = {\"Fluten\" : 0, \"Normalzustand\" : 1},\n",
    "                       is_training=False,\n",
    "                       transforms=None):\n",
    "        self.transforms = transforms\n",
    "        self.is_training = is_training\n",
    "        self.images = self.get_paths(root)\n",
    "        # indexed labels with key:vale pairs. Key being the parent folder containing the perticilar class images\n",
    "        self.labels = labels\n",
    "    \n",
    "    def get_paths(self, root):\n",
    "        img_format = ['.jpg', '.png']\n",
    "\n",
    "        dirs = [x[0] for x in os.walk(root, followlinks=True) if not x[0].startswith('.')]\n",
    "        datasets = []\n",
    "        for fdir in dirs:\n",
    "            for el in os.listdir(fdir):\n",
    "                if os.path.isfile(os.path.join(fdir, el)) and \\\n",
    "                not el.startswith('.') and \\\n",
    "                any([el.endswith(ext) for ext in img_format]):\n",
    "                    datasets.append(os.path.join(fdir,el))\n",
    "        shuffle(datasets)\n",
    "        return datasets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        image = Image.open(image)\n",
    "        if self.is_training:\n",
    "            if self.transforms is None:\n",
    "                self.transforms = transforms.Compose([\n",
    "                                                    transforms.Resize((256, 256)),\n",
    "                                                    transforms.RandomAffine([20,50]),\n",
    "                                                    transforms.RandomRotation([30,70]),\n",
    "                                                    transforms.RandomVerticalFlip(0.5),\n",
    "                                                    transforms.RandomHorizontalFlip(0.5),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize((0.5021, 0.4781, 0.4724), (0.3514, 0.3439, 0.3409)),\n",
    "                                                      ])    \n",
    "        else:\n",
    "            self.transforms = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor(),\n",
    "                                                transforms.Normalize((0.5021, 0.4781, 0.4724), (0.3514, 0.3439, 0.3409)),])\n",
    "        image = self.transforms(image)\n",
    "        return {'image' : image, 'label' : torch.tensor(self.labels[os.path.basename(os.path.dirname(self.images[index]))], dtype=torch.int64)}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e60534",
   "metadata": {},
   "source": [
    "### Dataset mean and standard deviation\n",
    "Below function is used to calculate the mean and standard deviation of the dataset.\n",
    "\n",
    "For this perticular task, the mean and standard deviation was computed to be:\n",
    "Mean : (0.5021, 0.4781, 0.4724), Std : (0.3514, 0.3439, 0.3409), for (r,g,b) channels respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac3d592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(dataloader):\n",
    "    channels_sum, channels_sum_squared, num_batches = 0, 0, 0\n",
    "    for data in tqdm(dataloader):\n",
    "        image = data['image']\n",
    "        channels_sum += torch.mean(image, dim=[0,2,3])\n",
    "        channels_sum_squared += torch.mean(image**2, dim=[0,2,3])\n",
    "        num_batches += 1\n",
    "    \n",
    "    mean = channels_sum / num_batches\n",
    "    std = (channels_sum_squared / num_batches - mean**2)**0.5\n",
    "    logging.info(f\"mean : {mean}, std : {std}\")\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a3b4bb",
   "metadata": {},
   "source": [
    "### Model\n",
    "The below cell defines model architecture. The model takes as input the images of size which is a multiple of 64. (We are using a default input shape of 256x256 in our dataloader).\n",
    "\n",
    "### Model Architecture\n",
    "![Model Architecture](https://drive.google.com/uc?export=view&id=1T5tcZlk0ZZv5j5iinWiCSLDz01rcxGma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b454fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import os\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                in_channels: int, \n",
    "                out_channels: int,\n",
    "                kernel_size: int=3, \n",
    "                stride: int=1,\n",
    "                padding=0,\n",
    "                bias: bool = True) -> None:\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        x = self.conv(inp)\n",
    "        x = self.bn(x)\n",
    "        return self.relu(x)\n",
    "\n",
    "class KeenModel(nn.Module):\n",
    "    def __init__(self, num_classes, input_shape, factor=4):\n",
    "        super(KeenModel, self).__init__()\n",
    "\n",
    "        input_channels = 3\n",
    "        # initial convolution\n",
    "        self.conv1 = ConvBlock(input_channels, 64//factor, stride=4, padding=1)\n",
    "        self.conv2 = ConvBlock(64//factor, 128 // factor, stride=2, padding=1)\n",
    "        self.conv3 = ConvBlock(128 // factor, 256 // factor, stride=2, padding=1)\n",
    "        self.conv4 = ConvBlock(256 // factor, 512 // factor, stride=2, padding=1)\n",
    "        self.avgpool = nn.AvgPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear((512//factor) * (input_shape//64) * (input_shape//64), 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                if m.bias is not None:\n",
    "                    init.uniform_(m.bias)\n",
    "                init.kaiming_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    \"\"\"\n",
    "    Downloads file from drive\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "    session = requests.Session()\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = None\n",
    "    for key, value in response.cookies.items():\n",
    "        if key.startswith('download_warning'):\n",
    "            token = value\n",
    "    if token:\n",
    "        params = { 'id' : id, 'confirm' : token }\n",
    "        response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "def keen_model(pretrained=True, num_classes=2, input_shape=256, factor=4):\n",
    "    \"\"\"\n",
    "    helper function to create model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = KeenModel(num_classes, input_shape, factor)\n",
    "        if pretrained:\n",
    "            checkpoint_path = os.path.join(os.getcwd(), \"checkpoint.pth\")\n",
    "            download_file_from_google_drive(\"1R06Oz4ZCcmHu3TGNG38raPfmVFYau2yy\", checkpoint_path)\n",
    "            checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "            if 'state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    except BaseException as e:\n",
    "        print(\"Error downloading checkpoint. {e}\")\n",
    "        print(\"Please download it manually from the following url : https://drive.google.com/file/d/1R06Oz4ZCcmHu3TGNG38raPfmVFYau2yy/view?usp=sharing\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ebca2",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b51ad",
   "metadata": {},
   "source": [
    "### Test model using pretrained weights.\n",
    "The pretrained weights for the model is available and unloaded under the google drive link :\n",
    "[checkpoint](https://drive.google.com/file/d/1R06Oz4ZCcmHu3TGNG38raPfmVFYau2yy/view?usp=sharing)\n",
    "\n",
    "\n",
    "To test the models performance using the pretrained weights, please use the function ```test_step(img_dir, ckpt_path)```,\n",
    "defined in the cell below. The function ```test_step(img_dir, ckpt_path)``` accepts the path to the root directory of the test dataset.\n",
    "\n",
    "If the checkpoint is available locally, pass its path as an argument to the ```ckpt_path``` parameter in ```test_step(img_dir, ckpt_path)``` function.\n",
    "\n",
    "If this argument is ```None```, the function tries to download the checkpoint from the google drive link provided above.\n",
    "\n",
    "In our validation process, our model acheved an Validation accuracy of 95 %. See the end of the notebook for plots showing training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "029697de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_config = {'batch_size': 16,\n",
    "               'num_workers': 0,\n",
    "               'pin_memory': True, \n",
    "               'drop_last': True}\n",
    "\n",
    "def test_step(img_dir, ckpt_path=None):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    if ckpt_path is not None:\n",
    "        model = KeenModel(2, 256)\n",
    "        # load checkpoint\n",
    "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "        if 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    else:\n",
    "        model = keen_model(True)\n",
    "    model = model.to(device)\n",
    "    # create criterion\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    # create testset\n",
    "    testset = KeenDataloader(img_dir, labels=label, is_training=False)\n",
    "    testloader = DataLoader(testset, **test_config)\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    iteration = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(testloader):\n",
    "        iteration+=1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data['image'].to(device), data['label'].to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        total += labels.size(0)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_loss += loss.item()\n",
    "    print('[%5d] Test loss: %.3f, Test Accuracy: %.3f' %\n",
    "                    (iteration, test_loss/(iteration+1e-5), correct/(total+1e-5)))\n",
    "    logging.info(f'Test completed')\n",
    "    return test_loss/(iteration+1e-5), correct/(total+1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a940e4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [04:22<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  198] Test loss: 0.101, Test Accuracy: 0.958\n",
      "0.1012925382737863 0.9577020171789709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss, acc = test_step(\"data/ValidationData\")\n",
    "print(loss, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4af7c17",
   "metadata": {},
   "source": [
    "### Predict on a single image\n",
    "There is a possibility to predict on a single image sample using the ```predict(image_path, ckpt_path)``` function. Just pass the path of the image as an argument and the function returns the predicted label.\n",
    "\n",
    "The loader and index are mapped as follows:\n",
    "* 0 : Fluten\n",
    "* 1 : Normalzustand\n",
    "\n",
    "Again pass the locally downloaded checkpoint path to the ```ckpt_path``` argument to use locally available checkpoint.\n",
    "\n",
    "If this argument is ```None```, the function tries to download the checkpoint from the google drive link provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d083988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, ckpt_path=None):\n",
    "    label = {0 : \"Fluten\", 1: \"Normalzustand\"}\n",
    "    # create model\n",
    "    if ckpt_path is not None:\n",
    "        model = KeenModel(2, 256)\n",
    "        # load checkpoint\n",
    "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "        if 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "    else:\n",
    "        model = keen_model(True)\n",
    "    # basic transformations\n",
    "    transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5021, 0.4781, 0.4724), (0.3514, 0.3439, 0.3409))])\n",
    "    # open image\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image)\n",
    "    image = torch.unsqueeze(image, dim=0)\n",
    "    model = model.eval() \n",
    "    outputs = model(image)\n",
    "    _, prediction = torch.max(outputs, dim=1)\n",
    "    probablities = torch.nn.functional.softmax(outputs.squeeze(), dim=0)\n",
    "    print(f\"Fluten : {probablities[0]*100} %, Normalzustand : {probablities[1]*100} %\")\n",
    "    return label[int(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c589d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fluten : 100.0 %, Normalzustand : 2.2069306104821607e-10 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Fluten'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"data/ValidationData/bad/fluten005339.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba15da9",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e37b9",
   "metadata": {},
   "source": [
    "### Run Configurations\n",
    "The following block contains the run configurations that can be set while training the model. \n",
    "\n",
    "We have trained the model with the configurations shown below:\n",
    "\n",
    "* 'train_path' : \"path/to/dataroot/Training\",\n",
    "*  'val_path' : \"path/to/dataroot/Validation\",\n",
    "*    'epochs' : 50,\n",
    "*    'lr' : 0.0001,\n",
    "*    'wd' : 0,\n",
    "*    'batch_size' : 16,\n",
    "*    'val_batch_size' : 16,\n",
    "*    'num_workers' : 0,\n",
    "*    'save_root' : \"path/to/savedirectory\",\n",
    "*    'checkpoint' : \"path/to/savedirectory/checkpoint\",\n",
    "*    'logs_root' : \"path/to/savedirectory/logs\",\n",
    "*    'resume' : \"path/to/checkpoint\",\n",
    "*    'print_freq' : 200,\n",
    "*    'save_freq' : 5,\n",
    "*    'val_freq' : 5,\n",
    "*    'initial_eval' : False,\n",
    "*    'is_training' : True\n",
    "\n",
    "Please cheange the values according to your requirements. If checkpoint path is None, the the model is initialized with random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26f8349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'train_path' : \"data/Training\",\n",
    "    'val_path' : \"data/Validation\",\n",
    "    'epochs' : 20,\n",
    "    'lr' : 0.0001,\n",
    "    'wd' : 0.0001,\n",
    "    'batch_size' : 64,\n",
    "    'val_batch_size' : 64,\n",
    "    'num_workers' : 0,\n",
    "    'save_root' : \"path/to/save/root\",\n",
    "    'checkpoint' : \"path/to/save/root/checkpoint\",\n",
    "    'logs_root' : \"path/to/save/root/logs\",\n",
    "    'resume' : \"path/to/checkpoint\",\n",
    "    'print_freq' : 100,\n",
    "    'save_freq' : 1,\n",
    "    'val_freq' : 2,\n",
    "    'initial_eval' : False,\n",
    "    'is_training' : True,\n",
    "    'lr_factor' : 0.1,\n",
    "    'lr_step_size' : 7,\n",
    "    'start_epoch' : 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3d8141",
   "metadata": {},
   "source": [
    "### Functions required for training\n",
    "The below cells contain the functions required for training the model\n",
    "\n",
    "### Functions to create dataloaders and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4044f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(config):\n",
    "    train_set = KeenDataloader(config['train_path'], is_training=True)\n",
    "    val_set = KeenDataloader(config['val_path'], is_training=False)\n",
    "    tkwargs = {'batch_size': config['batch_size'],\n",
    "               'num_workers': config['num_workers'],\n",
    "               'pin_memory': True, 'drop_last': True}\n",
    "    trainloader = DataLoader(train_set, **tkwargs)\n",
    "    tkwargs = {'batch_size': config['val_batch_size'],\n",
    "               'num_workers': config['num_workers'],\n",
    "               'pin_memory': True, 'drop_last': True}\n",
    "    valloader = DataLoader(val_set, **tkwargs)\n",
    "    return trainloader, valloader\n",
    "\n",
    "def create_model_and_optimizer():\n",
    "    model = KeenModel(2, 256).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['wd'])\n",
    "\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bc0b6e",
   "metadata": {},
   "source": [
    "### Functions to load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c21fad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model, optimizer, config):\n",
    "    if config[\"resume\"] is not None : \n",
    "        checkpoint = torch.load(config['resume'], map_location='cpu')\n",
    "        if 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "        if 'optimizer' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22afaa5e",
   "metadata": {},
   "source": [
    "### Functions to perform train step and val step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9bcaaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, criterion, optimizer, trainloader, epoch):\n",
    "    running_loss = 0.0\n",
    "    iteration = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(trainloader):\n",
    "        iteration+=1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data['image'].to(device), data['label'].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate acc\n",
    "        total += labels.size(0)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if iteration % config['print_freq'] == 0:\n",
    "            logging.info('[%d, %5d] loss: %.3f, accuracy: %.3f' %\n",
    "                (epoch + 1, iteration, running_loss/(iteration+1e-5), correct/(total+1e-5)))\n",
    "    logging.info('[%d, %5d] Epoch loss: %.3f, Accuracy: %.3f' %\n",
    "                    (epoch + 1, iteration, running_loss/(iteration+1e-5), correct/(total+1e-5)))\n",
    "    logging.info(f'Epoch {epoch} completed')\n",
    "    return running_loss/(iteration+1e-5), correct/(total+1e-5)\n",
    "\n",
    "def val_step(model, criterion, valloader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    iteration = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in tqdm(valloader):\n",
    "        iteration+=1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data['image'].to(device), data['label'].to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        total += labels.size(0)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        val_loss += loss.item()\n",
    "    logging.info('[%5d] Validation loss: %.3f, Validation Accuracy: %.3f' %\n",
    "                    (iteration, val_loss/(iteration+1e-5), correct/(total+1e-5)))\n",
    "    logging.info(f'Validation completed')\n",
    "    return val_loss/(iteration+1e-5), correct/(total+1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f90eb5",
   "metadata": {},
   "source": [
    "### Function to run train epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f3a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, model, criterion, optimizer, trainloader, valloader):\n",
    "    metrics = {'train_loss' : [], 'train_acc' : [], 'val_loss' : [], 'val_acc' : []}\n",
    "    model.train()\n",
    "    # le scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, config['lr_step_size'])\n",
    "    for epoch in range(config['start_epoch'], epochs):\n",
    "        logging.info(f\"Current learning rate : {scheduler.get_last_lr()}\")\n",
    "        # train a single epoch\n",
    "        train_loss, train_acc = train_step(model, criterion, optimizer, trainloader, epoch)\n",
    "        # validation\n",
    "        if epoch % config['val_freq'] == 0:\n",
    "            val_loss, val_accuracy = val_step(model, criterion, valloader)\n",
    "            metrics['val_loss'].append(val_loss)\n",
    "            metrics['val_acc'].append(val_accuracy)\n",
    "        # lr schedule\n",
    "        scheduler.step()\n",
    "        # update metrics\n",
    "        metrics['train_loss'].append(train_loss)\n",
    "        metrics['train_acc'].append(train_acc)\n",
    "        # save model\n",
    "        if epoch % config['save_freq'] == 0:\n",
    "            save_model(model, optimizer, epoch, config)\n",
    "            logging.info(f\"Model saved under : {os.path.join(config['save_root'], f'ckpt_epoch_{epoch}.pth')}\")\n",
    "    # save at the end of epoch\n",
    "    save_model(model, optimizer, epoch, config)\n",
    "    logging.info(f\"Model saved under : {os.path.join(config['save_root'], f'ckpt_epoch_{epoch}.pth')}\")\n",
    "    return metrics\n",
    "\n",
    "def save_model(model, optimizer, epoch, config):\n",
    "    # save checkpoint\n",
    "    model_optim_state = {'epoch': epoch,\n",
    "                         'state_dict': model.state_dict(),\n",
    "                         'optimizer': optimizer.state_dict(),\n",
    "                         }\n",
    "    model_name = os.path.join(\n",
    "        config['checkpoint'], 'ckpt_epoch_%03d_.pth' % (\n",
    "            epoch))\n",
    "    torch.save(model_optim_state, model_name)\n",
    "    logging.info('saved model {}'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c292b",
   "metadata": {},
   "source": [
    "## Steps to train the model\n",
    "Run the below cell to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config['save_root'], exist_ok=True)\n",
    "os.makedirs(config['logs_root'], exist_ok=True)\n",
    "os.makedirs(config['checkpoint'], exist_ok=True)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                filename=os.path.join(config['logs_root'], 'stdout.log'),\n",
    "                format='%(asctime)s %(message)s')\n",
    "logging.info('Run configurations:')\n",
    "for item in config:\n",
    "    logging.info(f\"{item} : {config[item]}\")\n",
    "logging.info(\"Creating dataloaders\")\n",
    "trainloader, valloader = create_dataloader(config)\n",
    "logging.info(\"Creating model, optimizer and criterion functions\")\n",
    "model, criterion, optimizer = create_model_and_optimizer()\n",
    "\n",
    "if config[\"initial_eval\"]:\n",
    "    val_loss = val_step(model, criterion, valloader)\n",
    "logging.info(f\"Training model on {device}:\")\n",
    "metrics = train(config['epochs'], model, criterion, optimizer, trainloader, valloader)\n",
    "with open(os.path.join(config['save_root'], 'metrics.pkl'), 'wb') as pkl:\n",
    "    pickle.dump(metrics, pkl, pickle.HIGHEST_PROTOCOL)\n",
    "logging.info('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d121fb",
   "metadata": {},
   "source": [
    "### Metrics on training.\n",
    "We have trained the model with the following main parameters:\n",
    "* Epochs : 20\n",
    "* Learning rate : 0.0001\n",
    "* Weight decay : 0.0001\n",
    "* Batch Size : 64\n",
    "* Loss : Cross Entropy\n",
    "* Optimizer : Adam\n",
    "\n",
    "The below graphs plots the accuracy and loss plots from the training and validation that was run.\n",
    "\n",
    "The values of accuracy and loss is stored as a python pickled file in the following link : \n",
    "[Metrics Pickled file](https://drive.google.com/file/d/1bfeE9c6u8Q299BFVuawrs5Xd8Ioa8wQu/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "792156c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAABJ9ElEQVR4nO3dd3xUVfr48c+TBBJCh4RiQkjogpAAoSgqIIgobGjSRKVYWQXRXcvaG2tjXcSCiw31i7CKgqAC0vG3CFIMiBRpkST0Hmra+f1xJ2EISZgkM3Mnmef9es1rZu7M3PPk5ibPnHNPEWMMSimlVF4BdgeglFLKN2mCUEoplS9NEEoppfKlCUIppVS+NEEopZTKV5DdAbhLWFiYiY6OtjsMVYatW7fusDEm3Nvl6rmtPKmw87rMJIjo6GjWrl1rdxiqDBORP+0oV89t5UmFndfaxKSUUipfmiCUUkrlq8w0ManSLz0d0tLg1Ck4cwZq1YIaNUDEtc+fPg1JSZCcDAcO5H87cQLCw6FuXbjiikvvGzSA6tU9+mOqEth0cBO3f3M7fZv15cH2DxIWGmZ3SGWaJogyIisL/vzT+idbHKdPQ0pK/rfDh61/mrVrX3qrVQsqVIDAQAgIsO6dH589C4cOWfvIe3/kyIWEkJaWf+wVK0L9+tYtKurC/dmzsHv3hduuXXDw4KWfDw29EGuDBlC1qlV+cjL88sulnxk/Hp58snjHUHnek4ufZMvhLWxYvoE3Vr7BXa3v4m9X/4361erbHVqZpAmiFDpyBH77DTZuvHDbtMn6p+kOQUEQEQGRkdC2LYSFwbFj1jfwnTvh55+tf7JFncZLBGrWtL7Bh4VBw4ZQpQpUqgSVK198CwmB/fthzx4r8e3ZY/1DP3Lk4jijoiAmBhISrAQQE2Nty0kKlSoVHlNGhvVz7d0L+/ZB06ZFP17KO9akrmHuH3N5uevL9LuyH6//73Umr53Me2veY8hVQ3is02O0qt3K7jDLFCkrk/XFx8ebstzTIzsb3nkH3njD+lafIywMYmOhVSto0cL6xl0cISFWQoiMtGoFAZe5OpWVZdUCDh6Ec+es+LKyrFvO4+xsCA6+kBCqV7dqFSVx6pT17T801EpiQV78iiMi64wx8d4r0VLWz21X9fqiF6tSVrH7od1UCa4CQPKJZP696t9MWTeF0xmnubnRzTze6XGur3894mrbpJ8r7LzWGkQpsG8fjBwJCxbADTfAQw9ZCaFVK+tbsh1/B4GBF76le1OlSnDlld4tU9lvVcoqftj+A690eyU3OQDUq1qPN296k6evf5r31rzHpNWT6PJpFzpEdODxTo/Tp1kfAkT74hSXHjkf9+23ViJYsQLeew8WLYK//x169IA6dexJDkp52/PLnicsNIwH2z+Y7+s1KtTg6euf5s9xf/LuLe9y8PRB+n/Zn+bvNuej9R9xPvO8lyMuG7QG4aNOn4ZHHoEpU6B1a5g2Tb85K/+0MnklC3Yu4PXur1OpfOEXlSqUq8Bf2/2Ve9vey8zNM3ntf69x99y7eXbZs4zrMI774u+7qAbiTcYYMrIzyMzOzL1lZF147vya8/a8r+X3uivvefK6J6lYvmht0HoNwgetXQvDhsH27fDoo/DSS1C+vN1RKb0GYY8bP7+RjQc2smvsriL/gzPGsHDXQl7732ss2b2EqsFVGdRiEDc3upluDbp5PFn8efxPvt/+PXP/mMvS3Us5n2VPTSYoIIjUR1KpVbHWJa/pNQgfdvQo7Nhh9Q7asQO2boUvv7SajxYvhq5d7Y5QKfus+HMFi3Yt4l89/lXk5AAgIvRo2IMeDXuwJnUNb656k+mbpvPB+g8ICgiiU71O9GzUk5sb3Uyr2q1KfGE7KzuLX1J/4bs/vmPuH3P57eBvADSu0Zj72t5H7Uq1CQoIIiggiHIB5XIfBwUEUS4wz/PLvO7qewIlsNg/l9YgvCg9HRYuhG++sbqp7thhdR91FhkJ3bvDm2/qgC1fozUI7+v6aVe2Ht7KzrE7CS0X6pZ9pmel83Pyz8zfMZ95O+ax4cAGAOpUqkPPRj3p2bAnNza8kRoVari0v7Tzafy480fm/jGXH7b/wKEzhwiUQK6rfx29G/fmL03/QpOaTdwSuydoDcJGGRmwZAn8978waxYcPw7VqkG7djBkCDRqZN0aNrT68VeoYHfE6nJEpCfwFhAIfGiMeTXP6/WBj4Fw4ChwuzEmxfFaFvCb4617jDEJXgu8lFm6eynLkpYx8aaJbksOAOUDy9M5ujOdozvzSvdX2Ju2lx93/si8HfOYvXU2UxOnEiABdIjoYCWMRj2JvyL+ot5Qu4/tzq0lLEtaRkZ2BtVDqnNz45vp3bg3PRv1pHqF0v8NT2sQHmAMLFsGM2bA119bg7sqV4a+fWHwYLjxRr2mUBqJyDqgA/AHcCOQAqwBhhpjNju97yvgO2PMpyJyAzDSGHOH47VTxpjLDN+7mC+d295ijKHz1M7sPLaTHWN2UKGcd745ZWZnsiZ1TW7tYu3etRgMYaFh9GjYgysqXcEPO35g8yHr190srFluLeGaetcQFFD6vnNrDcJLsrOt5qOXX4YNG6xBawkJMGgQ9OxpDUZTpV57YIcxZheAiMwA+gCbnd7THHjE8XgpMNubAZYFS3Yv4ac9P/HOze94LTmAdTH36npXc3W9q3mh6wscOn2IhbsWMn/HfObvmM+xc8e4vv713N36bv7S9C80qtHIa7HZQROEG2RlWU1I48fD5s3QpAl88omVGELdVzNWviECSHZ6noJVq3C2AeiP1QzVD6gsIjWNMUeAEBFZC2QCrxpjZudXiIjcC9wLEBUV5dYfwNcZY3h22bNEVonk7jZ32xpLeMVwbmt5G7e1vI1sk835zPNeTVh204FyJZCRAVOnWuMThg2zBq1Nn24liREjNDn4sb8DnUXkV6AzkApkOV6r76jO3wZMFJGG+e3AGDPFGBNvjIkPD/f6Ina2WrhrISuTV/LUdU8RHBRsdzi5AiTAr5IDaA2i2FassJLA7t0QFwczZ0K/fpefw0iVeqlAPafnkY5tuYwxe7FqEIhIJWCAMea447VUx/0uEVkGtAZ2ejzqUsIYw7NLnyWqahSjWo+yOxy/p//OiuHYMasHUkAAzJkD69fDgAGaHPzEGqCxiMSISHlgCDDH+Q0iEiaS2+XlH1g9mhCR6iISnPMeoBMXX7vwe/N3zGd16mqevu5pygdqTw67aQ2iGB56yJruevVqaNPG7miUNxljMkXkQWABVjfXj40xv4vIi8BaY8wcoAvwiogYYAXwgOPjVwL/EZFsrC9nrzr3fvJ3OdceYqrFMCJuhN3hKDRBFNmcOfD55/Dss5oc/JUx5gfghzzbnnV6PBOYmc/nVgItPR5gKfX99u9Zu3ctHyV8RLnAcnaHo9AmpiI5ehTuu89af+Gpp+yORqmyI+faQ8PqDbmj1R12h6MctAZRBGPHWovkzJunA92Ucqdvt33Lr/t/ZWqfqVp78CFag3DRt99aU24/9ZTVa0kp5R7ZJpvnlz1P4xqNGdZqmN3hKCdag3DBkSNW01JcnC5or5S7zdoyiw0HNvB5v89L5VQVZZn+NlwwdqyVJBYs0KYlpdwp22Tz/PLnaVqzKUOvGmp3OCoPTRCXMWsWfPEFvPCCdXFaKeU+MzfPZNPBTUwfMJ3AgEC7w1F5XPYahIj8xWnQj185fBjuv99a8vMf/7A7GqXKlqzsLF5Y/gLNw5szsPlAu8NR+XClBjEYa86Yr7EGBW31cExel54Oqanw55+wZ8+F+1WrrFHTCxdCOe1YoZRbffn7l2w+tJkvb/1Saw8+6rIJwhhzu4hUAYYCUx2jQz8Bphtj0jwdoCedOQMdO8KmTdYaDs5q14aoKPjgA2jVyp74lPJFGVkZ9Ptvv9xFdTpGdqRdRLsire+cU3toWaslA5oP8GC0qiRcugZhjDkpIjOBCsA4rCmMHxWRScaYtz0Yn0dt324t/TlsmLX2c/36VlKoV09XdlOqIDM3z+T77d9Tv2p95v4xFwBBuDL8SjpGdKRDZAc6RHSgRa0WBfZKmr5pOtuObOPrQV9ftFKb8i2XTRAikgCMBBoBnwHtjTEHRSQUa6KxUpsg9uyx7seOhfbt7Y1FqdLAGMO/V/2bJjWbsOWBLZw4d4JfUn9hdepqVqeu5ttt3/Jx4scAVCxXkfgr4nNrGR0iO3BF5SvIzM7kxeUvElcnjr7N+tr7A6lCuVKDGAD82xizwnmjMeaMiNxV2Ad9fe3enAThZ+uxKFVsq1JWsWbvGt65+R0CJIDqFapzU6ObuKnRTYCVQHYe28nqlNW5SePfq/5NRnYGAJFVIompFsP2o9uZPXi21h58nCsJ4nlgX84TEakA1DbGJBljFhf0IREJBN7Fae1eEZmTZ/bKCcBnTmv3vgLkTMRy1hgTV5QfpqiSk62Lz7VqebIUpcqOt1a/RdXgqgyPG57v6yJCoxqNaFSjUe6o6HOZ50jcn3hR0rixwY0kNHX7dz7lZq4kiK+Aa5yeZzm2tbvM53x+7d49e6zrDbqOg1KXl3wimZmbZ/Jwx4epVL6Sy58LCQqhY2RHOkZ29GB0yhNc+dcYZIxJz3nieOzKeOL81u6NyPOenLV7wWntXsfzEBFZKyKrRKSvC+UV2Z492ryklKveXfMuBsOD7R+0OxTlJa4kiEOOC9UAiEgf4LCbyi/R2r0icq8jiaw9dOhQkQtPTrZqEEqpwp1OP82UdVPo16wf9avVtzsc5SWuNDHdD0wTkXcAwaoV3OnC5zy+dq8xZgowBSA+Pj7PSIbCZWZag+O0BqHU5f3fxv/j2LljjOs4zu5QlBe5MlBuJ9DR8Q8cY8wpF/edu3YvVmIYglUbyOVYl/eoMSabPGv3AmeMMeed1u593cVyXbJvH2RlaYJQ6nKyTTYTV0+kbd22dKrXye5wlBe5NFBORHoBLbCuCwBgjHmxsM/4+tq9yY6rI9rEpFThFu5cyNbDW/ms72fk/P0r/+DKQLn3gVCgK/AhcCvwiys79+W1e3UMhFKueWv1W9SpVIdBLQbZHYryMlcuUl9jjLkTOGaMeQG4Gmji2bA8LydBaA1CqYJtPbyVeTvm8df4vxIcFGx3OMrLXEkQ5xz3Z0TkCiADqOu5kLwjORmqVoUqrs8vppTfmbR6EsGBwdwXf5/doSgbuHINYq6IVAPeANYDBvjAk0F5g46BUKpwx84e49MNn3Jby9uoVVGnG/BHhSYIx0JBix1dT78Wke+AEGPMCW8E50maIJQq3IfrP+RMxhke6vCQ3aEomxTaxOTofvqu0/PzZSE5gA6SU6owmdmZvP3L23SN7kpsHV1r11+5cg1isYgMkDLUv+30aThyRGsQShVk1pZZJJ9M1tqDn3MlQdyHNTnfeRE5KSJpInLSw3F5VM4YCE0QSuVv4uqJNKjegN5NetsdirKRKyOpK3sjEG/SQXJKFWxN6hpWJq9k4k0Tda1oP+fKQLnr89uedwGh0kQHySlVsLdWv0Xl8pUZ2Xqk3aEom7nSzfVRp8chWOs8rANu8EhEXrBnD4hARN7Jx5Xyc3vT9vLf3//LA+0eoEqwDhLyd640Mf3F+bmI1AMmeiogb0hOhrp1rdXklFIXvLfmPbKysxjTfozdoSgfUJy11FKwJtMrtXQMhCopEekpIttEZIeIPJHP6/VFZLGIbBSRZSIS6fTacBHZ7rjlv3anDc5mnOU/6/5DQtMEGta4ZPkV5YdcuQbxNtboabASShzWiOpSa88eiIuzOwpVWpVkvXURqQE8B8Rj/V2tc3z2mHd/ikt98dsXHD5zWNd8ULlcqUGsxbrmsA74GXjcGHO7R6PyIGOsJiatQagSyF1v3bEEb856686aA0scj5c6vX4TsNAYc9SRFBYCPb0Qc6GMMUxcPZFWtVvRuX5nu8NRPsKVi9QzgXPGmCywvj2JSKgx5oxnQ/OMw4fh3DlNEKpE8ltvvUOe9+Sst/4WF6+37spa7YjIvcC9AFFeOFmXJi1l08FNfJzwsa75oHK5NJIaqOD0vAKwyDPheJ6OgVBeUth665dljJlijIk3xsSHh4d7KsZcE1dNJDw0nKEth3q8LFV6uJIgQpyXGXU8DvVcSJ6lYyCUG7i03roxpr8xpjXwlGPbcVc+623bj2znuz++Y3T8aEKCQuwMRfkYVxLEaRFpk/NERNoCZz0XkmdpglBukLveuoiUx1pvfY7zG0QkzDEbMjitt461BG8PEanuWHu9h2Obbd7+5W2CAoIY3W60nWEoH+TKNYhxwFcishcQoA4w2JNBeVJyMoSEQFiY3ZGo0qok660bY46KyEtYSQbgRWPMUa//EA4nzp3gk8RPGHLVEOpUqmNXGMpHuTJQbo2INAOaOjZtM8ZkeDYsz9mzx7r+oNfhVEkUd711x2sfc6FGYauPf/2YU+mndNZWla/LNjGJyANARWPMJmPMJqCSiPzV86F5hg6SU8qSlZ3FpF8mcW3UtbS9oq3d4Sgf5Mo1iHscF9cAcPTdvsdjEXmYLhSklGXOtjkkHU9iXIdxdoeifJQrCSLQebEgxyjS8p4LyXMyMmDvXq1BKAXWrK31q9anT7O8Y/yUsriSIOYD/xWRbiLSDZgOzPNsWJ6RmmqNpNYEofzdr/t+ZfmfyxnTfgxBAa70VVH+yJUz43GsEZ33O55vxOrJVOroIDmlLE8teYqqwVW5q81ddoeifNhlaxDGmGxgNZCENQfNDcAWz4blGToGQilYuHMh83bM4+nrn6ZaSDW7w1E+rMAahIg0AYY6boeB/wIYY7p6JzT3y0kQWoNQ/iorO4u/L/w70dWidc0HdVmFNTFtBX4CehtjdgCIyMNeicpDkpOhRg2oWNHuSJSyx6cbPmXjgY3MGDCD4KBgu8NRPq6wJqb+wD5gqYh84LhAXaqHl+kYCOXPTqWf4uklT9MxsiODWgyyOxxVChSYIIwxs40xQ4BmWPPZjwNqichkEenhys59bdWtnFHUSvmjf638F/tO7eNfPf6lU3orl7hykfq0MeYLx9rUkcCvWD2bCuW06tbNWIunDBWR5nnelrPqVivgRaxVt3BadasD1oXx5xwTm5WILhSk/NXetL28vvJ1BjYfyDX1rrE7HFVKFGlNamPMMcc89d1ceLtPrbp18iQcP64JQvmnZ5Y8Q0ZWBq90e8XuUFQpUqQEUUSurJyVs+oWFHPVLRFZKyJrDx06VGgwOgZC+asN+zfwSeInjGk/hoY1GtodjipF7B5C+XfgHREZgTUlcpFX3QKmAMTHx5vC3puTIOyoQWRkZJCSksK5c+e8X7gqspCQECIjIylXrpzdoRSoKOfUsVPHmH/TfCKqRLBlS6kcwqRc5O5z15MJwqVVt3DUIESkEjDAGHNcRFKx5tN3/uyykgRj5yC5lJQUKleuTHR0tF4c9HHGGI4cOUJKSgoxMTF2h1MgV8+pE+dOcProaepVqUftSrW9GKHyNk+cu55sYvKpVbf27IGAAKhbtyR7KZ5z585Rs2ZNTQ6lgIhQs2ZNn6/tuXJOGWNIPplMcGAw4RU9v661spcnzl2PJQhjTCaQs+rWFuDLnFW3RCTB8bYuwDYR+QOoDYx3fPYokLPq1hrcsOpWcjJERECQTY1qmhxKj9Lyu7pcnIfPHOZc5jkiq0QSIJ78Lqh8hbvPXY/+u/SlVbd0kJzyJ1nZWaSmpVKpfCWdb0kVm998rfDnQXJHjhwhLi6OuLg46tSpQ0RERO7z9PT0Qj+7du1axo4dW+QyExMTERHmz59f3LBVCew/tZ/M7EzqVannkRpR165dWbDg4lbfiRMnMnr06AI/06VLF9auXQvALbfcwvHjxy95z/PPP8+ECRMKLXv27Nls3rw59/mzzz7LokWLihB94caNG0dERATZ2dlu22dp5RcJIjsbUlL8twZRs2ZNEhMTSUxM5P777+fhhx/OfV6+fHkyMzML/Gx8fDyTJk0qcpnTp0/n2muvZfr06SUJ/bKyslzu9OY30jPT2X96PzUq1KBiec9MPDZ06FBmzJhx0bYZM2YwdOhQlz7/ww8/UK1atWKVnTdBvPjii3Tv3r1Y+8orOzubWbNmUa9ePZYvX+6WfeansL85X+IXCeLgQUhP998EkZ8RI0Zw//3306FDBx577DF++eUXrr76alq3bs0111zDtm3bAFi2bBm9e/cGrG93o0aNokuXLjRo0KDAxGGM4auvvmLq1KksXLjwootmr732Gi1btiQ2NpYnnrBmX9mxYwfdu3cnNjaWNm3asHPnzovKBXjwwQeZOnUqANHR0Tz++OO0adOGr776ig8++IB27doRGxvLgAEDOHPmDAAHDhygX79+xMbGEhsby8qVK3n22WeZOHFi7n6feuop3nrrLbcdV1+QmpYKBiIqXzJ0yG1uvfVWvv/++9waaFJSEnv37uW6665j9OjRxMfH06JFC5577rl8Px8dHc3hw4cBGD9+PE2aNOHaa6/NPe+AfH+vK1euZM6cOTz66KPExcWxc+dORowYwcyZVkv14sWLad26NS1btmTUqFGcP38+t7znnnuONm3a0LJlS7Zu3ZpvXMuWLaNFixaMHj36oi83+Z1LAJ999hmtWrUiNjaWO+64A+CieAAqVaqUu+/rrruOhIQEmje3JpXo27cvbdu2pUWLFkyZMiX3M/Pnz6dNmzbExsbSrVs3srOzady4MTnjvbKzs2nUqBGXG/9VUnaPg/AKX5rme9w4SEx07z7j4sDpf57LUlJSWLlyJYGBgZw8eZKffvqJoKAgFi1axJNPPsnXX399yWe2bt3K0qVLSUtLo2nTpowePfqSPtcrV64kJiaGhg0b0qVLF77//nsGDBjAvHnz+Pbbb1m9ejWhoaEcPWr1Oxg2bBhPPPEE/fr149y5c2RnZ5OcnHxJ2c5q1qzJ+vXrAasJ7Z57rGXSn376aT766CPGjBnD2LFj6dy5M7NmzSIrK4tTp05xxRVX0L9/f8aNG0d2djYzZszgl19+KfrB8yHj5o8jcX8iAFkmizMZZygfWJ7gwOLP1hpXJ46JPScW+HqNGjVo37498+bNo0+fPsyYMYNBgwYhIowfP54aNWqQlZVFt27d2LhxI61atcp3P+vWrWPGjBkkJiaSmZlJmzZtaNu2LQD9+/fP9/eakJBA7969ufXWWy/a17lz5xgxYgSLFy+mSZMm3HnnnUyePJlx48YBEBYWxvr163nvvfeYMGECH3744SXxTJ8+naFDh9KnTx+efPJJMjIyKFeuXL7n0u+//87LL7/MypUrCQsLyz2fC7N+/Xo2bdqU2w31448/pkaNGpw9e5Z27doxYMAAsrOzueeee1ixYgUxMTEcPXqUgIAAbr/9dqZNm8a4ceNYtGgRsbGxhId7tneaX9Qg7Bwk58sGDhxIYGAgACdOnGDgwIFcddVVPPzww/z+++/5fqZXr14EBwcTFhZGrVq1OHDgwCXvmT59OkOGDAFgyJAhud/EFi1axMiRIwkNDQWsfzJpaWmkpqbSr18/wBrok/N6YQYPHpz7eNOmTVx33XW0bNmSadOm5ca+ZMmS3DbxwMBAqlatSnR0NDVr1uTXX3/lxx9/pHXr1tSsWdOl41UanM88jyCUD/T8svHOzUzOzUtffvklbdq0oXXr1vz+++8XNQfl9dNPP9GvXz9CQ0OpUqUKCQkJua8V9HstyLZt24iJiaFJkyYADB8+nBUrVuS+3r+/NWlD27ZtSUpKuuTz6enp/PDDD/Tt25cqVarQoUOH3Oss+Z1LS5YsYeDAgYSFhQHW+Xw57du3v2iMwqRJk4iNjaVjx44kJyezfft2Vq1axfXXX5/7vpz9jho1is8++wywEsvIkSMvW15JaQ3Cy4rzTd9TKjotjPHMM8/QtWtXZs2aRVJSEl26dMn3M8HBF76VBgYGXtKWmpWVxddff823337L+PHjcwfvpKWlFSm2oKCgiy4S5u3b7Rz7iBEjmD17NrGxsUydOpVly5YVuu+7776bqVOnsn//fkaNGlWkuHxRzjf94+eOs+PoDqKqRlGrYi2Pl9unTx8efvhh1q9fz5kzZ2jbti27d+9mwoQJrFmzhurVqzNixIhi98sv6u/1cnLO3fzOW4AFCxZw/PhxWrZsCcCZM2eoUKHCRU2drnA+d7Ozsy/qCOJ83i5btoxFixbx888/ExoaSpcuXQo9VvXq1aN27dosWbKEX375hWnTphUpruLwmxpEaKi1WJDK34kTJ4iIsNqsc9r6i2Px4sW0atWK5ORkkpKS+PPPPxkwYACzZs3ixhtv5JNPPsm9RnD06FEqV65MZGQks2fPBuD8+fOcOXOG+vXrs3nzZs6fP8/x48dZvHhxgWWmpaVRt25dMjIyLvqj6datG5MnTwasxHXixAkA+vXrx/z581mzZg033XRTsX9WX5Jtskk5mUJIUAhhoWFeKbNSpUp07dqVUaNG5dYeTp48ScWKFalatSoHDhxg3rx5he7j+uuvZ/bs2Zw9e5a0tDTmzp2b+1pBv9fKlSvn+4WjadOmJCUlsWPHDgA+//xzOnfu7PLPM336dD788EOSkpJISkpi9+7dLFy4kDNnzuR7Lt1www189dVXHDlyBCC3iSk6Opp169YBMGfOHDIyMvIt78SJE1SvXp3Q0FC2bt3KqlWrAOjYsSMrVqxg9+7dF+0XrC83t99++0W1f0/yiwSRMwailIx/ssVjjz3GP/7xD1q3bl2iHhbTp0/PbS7KMWDAAKZPn07Pnj1JSEggPj6euLi43O6Mn3/+OZMmTaJVq1Zcc8017N+/n3r16jFo0CCuuuoqBg0aROvWrQss86WXXqJDhw506tSJZs2a5W5/6623WLp0KS1btqRt27a5TR3ly5ena9euDBo0yCt/ZN5g16C4oUOHsmHDhtwEERsbS+vWrWnWrBm33XYbnTp1KvTzbdq0YfDgwcTGxnLzzTfTrl273NcK+r0OGTKEN954g9atW7Nz587c7SEhIXzyyScMHDiQli1bEhAQwP333+/Sz3HmzBnmz59Pr169crdVrFiRa6+9lrlz5+Z7LrVo0YKnnnqKzp07ExsbyyOPPALAPffcw/Lly4mNjeXnn3++qNbgrGfPnmRmZnLllVfyxBNP0LFjRwDCw8OZMmUK/fv3JzY29qLm1ISEBE6dOuWV5iXA6nFSFm5t27Y1BWnXzpgbbyzwZY/bvHmzfYWrS2RlZZnY2Fjzxx9/FPie/H5nwFrjI+e2c3wZWRnm132/mq2Htprs7GyXj4MqfdasWWOuvfbaQt9T1P83hZ3XflGD0IWCVI7NmzfTqFEjunXrRuPGje0Oxy08PShO+YZXX32VAQMG8Mor3lvTo8xfpD5/Hvbv1wShLM2bN2fXrl12h+E25zPPc+DUAWpWqElo+cv3/lKl1xNPPJE7dshbynwNIiXFure7B5NVk1OlQWn5XRljSE1LRUSIqOK5QXGq9HD3uVvmE4QvjIEICQnhyJEjpeYfjz8zjm65ISEhdodSqJCQEFIPpHL0zFFqV6ztlXEPyrd54twt801Mdi4UlCMyMpKUlBSPD4tX7pGzKldhRKQn8BYQCHxojHk1z+tRwKdANcd7njDG/CAi0VjT3+fMKbHKGONaVxsnERERfLnqS8LLh1OhSgVOysmi7kKVQa6cu0VR5hNE69bwz3/a28RUrlw5n16dTBWNiAQC7wI3Yq2XvkZE5hhjnIcMP421BspkEWmONe19tOO1ncaYuJLEUK5cOWrUrcGpjFO0aN6iJLtSqkBlPkG0bGndlHKj9sAOY8wuABGZAfQBnBOEAao4HlcF9rozABHhlsa3uHOXSl2izF+DUMoDIgDn2QRTHNucPQ/cLiIpWLWHMU6vxYjIryKyXESuy68AEblXRNaKyFptmlR20QShlGcMBaYaYyKBW4DPHeuv7wOijDGtgUeAL0SkSt4PG2OmGGPijTHxnp6xU6mClJkmpnXr1h0WkT8LeDkMOOzNeAqhsVzKV+KAwmOp77hPBZyvakU6tjm7C+gJYIz5WURCgDBjzEHgvGP7OhHZCTQB1hYUkA+f23aV7Y8/syfLrl/QC2UmQRhjCvyaJSJrjTHx3oynIBqL78YBLseyBmgsIjFYiWEIcFue9+wBugFTReRKIAQ4JCLhwFFjTJaINAAaA4WO3PPVc9uusv3xZ7ar7DKTIJTyFmNMpog8CCzA6sL6sTHmdxF5EWtemznA34APRORhrAvWI4wxRkSuB14UkQwgG7jfGHP5lWaUsoEmCKWKwRjzA9bFZ+dtzzo93gxcMpWpMeZr4NKl+pTyQf5ykXrK5d/iNRrLpXwlDvCtWFxhZ7x2le2PP7MtZYtO/6CUUio//lKDUEopVUSaIJRSSuWrzCcIEekpIttEZIeIeHcy9UtjSRKR30QkUUQK7PfugXI/FpGDIrLJaVsNEVkoItsd99VtjOV5EUl1HJdEEfH4HBIiUk9ElorIZhH5XUQecmy35bgUlV3ndUHHzZtEJNAxEv07L5dbTURmishWEdkiIld7qdyHHcd6k4hMd4yp8YoynSCcJlW7GWgODHVMnGanrsaYOC/3Z56KY9CWkyeAxcaYxsBix3O7YgH4t+O4xDl6CHlaJvA3Y0xzoCPwgOPcsOu4uMzm87qg4+ZND2HNiOttbwHzjTHNgFhvxCAiEcBYIN4YcxVWt+ohni43R5lOEDhNqmaMSQdyJlXzK8aYFUDevvZ9sKajxnHf18ZYvM4Ys88Ys97xOA3rjz0Cm45LEdl2Xhdy3LxCRCKBXsCH3irTUW5V4HrgIwBjTLox5riXig8CKohIEBCKmyd+LExZTxCuTKrmTQb4UUTWici9NsYBUNsYs8/xeD9Q285ggAdFZKOjCcqrzTqONRpaA6vxveOSH584r/McN2+ZCDyGNcjQm2KAQ8AnjuatD0WkoqcLNcakAhOwRubvA04YY370dLk5ynqC8DXXGmPaYDUNPOAYVWs7Y/V1trO/82SgIRCH9UfwL28VLCKVsAaujTPGXLTqjg8cF59V2HHzYJm9gYPGmHXeKC+PIKANMNkx0eJpvND86Piy1AcrQV0BVBSR2z1dbo6yniBcmVTNaxzfBnBM2DYLq6nALgdEpC6A4/6gXYEYYw4YY7KMMdnAB3jpuIhIOax/ctOMMd84NvvMcSmEred1AcfNGzoBCSKShNWsdoOI/J+Xyk4BUowxObWlmVgJw9O6A7uNMYeMMRnAN8A1XigXKPsJIndSNREpj3VxZ44dgYhIRRGpnPMY6AFsKvxTHjUHGO54PBz41q5Acv4hO/TDC8dFRASrPXmLMeZNp5d85rgUwrbzupDj5nHGmH8YYyKNMdFYP/MSY4xXvk0bY/YDySLS1LGpGxcvEOUpe4COIhLqOPbd8OIF+jI9F1NBk6rZFE5tYJb1OyYI+MIYM98bBYvIdKALECbWAjbPAa8CX4rIXcCfwCAbY+kiInFYzTlJwH1eCKUTcAfwm4gkOrY9iU3HpShsPq/zPW5e6nlmtzHANEdS3gWM9HSBxpjVIjITWI/Vg+xXvDjlhk61oZRSKl9lvYlJKaVUMdmSIC43ClRE7ncacfz/fGBwm1JK+R2vNzE5RoH+AdyI1TNgDTDUMX9+znuq5HSdE5EE4K/GmPxG3yqllPIQOy5S544CBRCRnFGguQkiT7/qirjQFz0sLMxER0e7N1KlnKxbt+5wYct/eoqe28qTCjuv7UgQ+Y0C7ZD3TSLyAPAIUB64Ib8dOUYj3wsQFRXF2rVem/9O+SER+dOOcqOjo/XcVh5T2HntsxepjTHvGmMaAo8DTxfwninGmHhjTHx4uNe/2CmlVJlmR4Io6ijQGZRkwrTUVPjmG8jMLPYulPJJxzfBUTtmnVD+wo4EcdlRoCLS2OlpL2B7sUubPx8GDIDk5Mu/V6nSwmTDyttg+V/gjG2zx6gyzuvXIAoaBSoiLwJrjTFzsGb27A5kAMe4MPVB0TVoYN3v2gUxMSWM3jdlZGSQkpLCuXPn7A6lTAgJCSEyMpJy5crZHUrBJACumQY/XgMr+kD3FRAU6rXi9ZwrfYpzXtsy1YZjWP4PebY96/TYfatU5SSF3bvdtktfk5KSQuXKlYmOjsYxlYcqJmMMR44cISUlhRhf/0JRrSVc84WVIFaNhE4zwEu/fz3nSpfintc+e5HabSIjITCwTCeIc+fOUbNmTf1DdQMRoWbNmqXnm3HkXyDuVdjzJWx6yWvF6jlXuhT3vC7Tk/UBEBQE9etbTUxlmP6huk+pO5ZXPgonfoffnoOqV0LUQK8UW+qOk58rzu+r7NcgwGpmKsM1COXnRKD9FAi7Bn4erj2blNv4R4Jo0KDM1yDsdOTIEeLi4oiLi6NOnTpERETkPk9PTy/0s2vXrmXs2LFFKi86OprDhw+XJOSyJzAYrp8FweGwvA+c3Xf5z5RiXbt2ZcGCBRdtmzhxIqNHjy7wM126dMkdcHjLLbdw/PjxS97z/PPPM2HChELLnj17Nps3X1gK4tlnn2XRokVFiD5/y5Yto3fv3iXejzv5R4KIiYFDh+DUKbsjKZNq1qxJYmIiiYmJ3H///Tz88MO5z8uXL09mIWNQ4uPjmTRpkhejLcNCakHnuZBx3EoSmWftjshjhg4dyowZMy7aNmPGDIYOHerS53/44QeqVatWrLLzJogXX3yR7t27F2tfvq7sX4OACz2ZkpLgqqtsDcXjxo2DxET37jMuDiZOLNJHRowYQUhICL/++iudOnViyJAhPPTQQ5w7d44KFSrwySef0LRpU5YtW8aECRP47rvveP7559mzZw+7du1iz549jBs3zuXaRVJSEqNGjeLw4cOEh4fzySefEBUVxVdffcULL7xAYGAgVatWZcWKFfz++++MHDmS9PR0srOz+frrr2ncuPHlCykNqreyur+u6AerR1m9nDx9rWDdODiW6N59Vo+DthMLfPnWW2/l6aefJj09nfLly5OUlMTevXu57rrrGD16NGvWrOHs2bPceuutvPDCC5d8Pmf6krCwMMaPH8+nn35KrVq1qFevHm3btgXggw8+YMqUKaSnp9OoUSM+//xzEhMTmTNnDsuXL+fll1/m66+/5qWXXqJ3797ceuutLF68mL///e9kZmbSrl07Jk+eTHBwMNHR0QwfPpy5c+eSkZHBV199RbNmzVw6FNOnT+ef//wnxhh69erFa6+9RlZWFnfddRdr165FRBg1ahQPP/wwkyZN4v333ycoKIjmzZtfkkSLyj8ShPNYiLKeIHxISkoKK1euJDAwkJMnT/LTTz8RFBTEokWLePLJJ/n6668v+czWrVtZunQpaWlpNG3alNGjR7vUb3vMmDEMHz6c4cOH8/HHHzN27Fhmz57Niy++yIIFC4iIiMhtUnj//fd56KGHGDZsGOnp6WRlZbn7R7dXZB+I/Sds+AdUbQFX5TtTTalWo0YN2rdvz7x58+jTpw8zZsxg0KBBiAjjx4+nRo0aZGVl0a1bNzZu3EirVq3y3c+6deuYMWMGiYmJZGZm0qZNm9wE0b9/f+655x4Ann76aT766CPGjBlDQkJCbkJwdu7cOUaMGMHixYtp0qQJd955J5MnT2bcuHEAhIWFsX79et577z0mTJjAhx9+eNmfc+/evTz++OOsW7eO6tWr06NHD2bPnk29evVITU1l0yZrdd6cc/vVV19l9+7dBAcH59uEVlT+kSD8YCxEriJ+0/ekgQMHEhgYCMCJEycYPnw427dvR0TIyMjI9zO9evUiODiY4OBgatWqxYEDB4iMjLxsWT///DPffPMNAHfccQePPfYYAJ06dWLEiBEMGjSI/v37A3D11Vczfvx4UlJS6N+/f9mpPThr/jic2Awbn4EqV0LUAM+VVcg3fU/KaWbKSRAfffQRAF9++SVTpkwhMzOTffv2sXnz5gITxE8//US/fv0IDbUGGSYkJOS+tmnTJp5++mmOHz/OqVOnuOmmmwqNZ9u2bcTExNCkSRMAhg8fzrvvvpubIHLOv7Zt2+aeq5ezZs0aunTpQs5cc8OGDWPFihU888wz7Nq1izFjxtCrVy969OgBQKtWrRg2bBh9+/alb9++LpVRGP+4BhEWBpUq6YVqL6tYsWLu42eeeYauXbuyadMm5s6dW2B/7ODg4NzHgYGBhV6/cMX777/Pyy+/THJyMm3btuXIkSPcdtttzJkzhwoVKnDLLbewZMmSEpXhk0SgwxSo2RF+vhOO/mp3RG7Xp08fFi9ezPr16zlz5gxt27Zl9+7dTJgwgcWLF7Nx40Z69epV7DEtI0aM4J133uG3337jueeeK/HYmJxz2x3ndfXq1dmwYQNdunTh/fff5+677wbg+++/54EHHmD9+vW0a9euxOX4R4IQ0a6uNjtx4gQREREATJ061e37v+aaa3LbW6dNm8Z1110HwM6dO+nQoQMvvvgi4eHhJCcns2vXLho0aMDYsWPp06cPGzdudHs8PiEwBK6fDcE1YUVCmevZVKlSJbp27cqoUaNyL06fPHmSihUrUrVqVQ4cOMC8efMK3cf111/P7NmzOXv2LGlpacydOzf3tbS0NOrWrUtGRgbTpk3L3V65cmXS0tIu2VfTpk1JSkpix44dAHz++ed07ty5RD9j+/btWb58OYcPHyYrK4vp06fTuXNnDh8+THZ2NgMGDODll19m/fr1ZGdnk5ycTNeuXXnttdc4ceIEp0rYMcc/mpjAShBag7DNY489xvDhw3n55Zfp1atXiffXqlUrAgKs7zeDBg3i7bffZuTIkbzxxhu5F6kBHn30UbZv344xhm7duhEbG8trr73G559/Trly5ahTpw5PPvlkiePJISI9gbew5hn70Bjzap7X7wceALKAU8C9zqspul2F2tB5DvzYybpw3X2ZlTjKiKFDh9KvX7/cLwexsbG0bt2aZs2aUa9ePTp16lTo59u0acPgwYOJjY2lVq1atGvXLve1l156iQ4dOhAeHk6HDh1yk8KQIUO45557mDRpEjNnzsx9f0hICJ988gkDBw7MvUh9//33F+nnWbx48UVNql999RWvvvoqXbt2zb1I3adPHzZs2MDIkSPJzs4G4JVXXiErK4vbb7+dEydOYIxh7Nixxe6plcsYUyZubdu2NYUaN86Y0FBjsrMLf18ptHnzZrtDKHPyO6ZYk0kWeA5iJYWdQAOsha42AM3zvKeK0+MEYH5h+zSunNuu2PONMdMw5v/d5pa/AT3nSqeintf+0cQEVg3izBlrPIRSnpG7nK4xJh1rLZM+zm8wxVhO1y3q9YPY8fDnF7D5Fa8UqUo//0kQzl1dlfKM/JbTjcj7JhF5QER2Aq8D+Q70EJF7RWStiKw95K4vNc3/AdHDYMNTkDzLPftUZZr/JIgy3tXVqikqd/D0sTR2LacrAh0+hJodYOXtJR7cpudc6VKc35f/JIjoaOu+DCaIkJAQjhw5on+wbmAc8+aHhBTrQq53l9MtjtyeTTVgeQJkni7WbvScK12Ke177Ty+mihWhdu0y2cQUGRlJSkoKbmuK8HM5K28VQ+5yuliJYQhwm/MbRKSxMSZnCd2SLadbXBXqQIePYWkPSP0O6g8u8i70nCt9inNe+0+CgDI7FqJcuXK+v/qZHzDeXk63JGrfABXqwp//LVaC0HPOP/hXgmjQAFautDsKVYYZby6nWxIBgVDvVtgxBTLSoFxluyNSPsh/rkGAVYNIToYSDj9XqkyoPxiyz0PKHLsjUT7K/xJEVpaVJJTyd2FXQ2gk7Pmv3ZEoH+VfCULHQih1gQRA1CDYtwDSj9sdjfJB/pUgyvhYCKWKLGoQZKdDyrd2R6J8kH8liMhICArSGoRSOWq2h4rRVm8mpfLwrwQRFARRUVqDUCqHiFWL2L8Qzh+xOxrlY2xJECLSU0S2icgOEXkin9cfEZHNIrJRRBaLSH23FV5Gx0IoVWz1B4HJhJTZdkeifIzXE4SIBALvAjcDzYGhItI8z9t+BeKNMa2AmViTmrlHgwbaxKSUs+ptoFJDbWZSl7CjBuHKlMhLjTFnHE9XYc1p4x4xMdaU3yVcaUmpMkPEGhNxYAmc06kz1AV2JAiXpkR2cheQ77qBxZoSOaerqzYzKXVB1CAwWZD8jd2RKB/i0xepReR2IB54I7/XizUlsnZ1VepS1VpBlaY6aE5dxI4E4dKUyI4JzZ4CEowx591WuiYIpS4lAlGD4eByOLvf7miUj7AjQeROiSwi5bGmRL5oMhgRaQ38Bys5HHRr6WFhUKmSXqhWKq+oQWCyIflruyNRPsLrCcIYkwnkTIm8BfgyZ0pkEUlwvO0NoBLwlYgkioj7ZhMT0a6uSuWnWguo2kJ7M6lctkz37cKUyN09GkCDBrBzp0eLUKpUihoMvz0HZ1IhtLC+I8oflLgGISIVRSTA8biJiCSISLmSh+ZBMTFWE5Mul6jUxeoPAgzsmWl3JMoHuKOJaQUQIiIRwI/AHcBUN+zXc2Ji4MwZazyEUuqCKk2hWqz2ZlKAexKEOAa19QfeM8YMBFq4Yb+eo9N+K1Ww+oPh8M9weo/dkSibuSVBiMjVwDDge8e2QDfs13O0q6tSBYsaZN3v+dLeOJTt3JEgxgH/AGY5eiM1AJa6Yb+eowlCqYJVbgg12sKfmiD8XYkThDFmuTEmwRjzmuNi9WFjzFg3xOY5oaFQu7Y2MSlVkKjBcHQNnNK/EX/mjl5MX4hIFRGpCGwCNovIoyUPzcN0LIRSBavvaGbSWoRfc0cTU3NjzEmgL9akejFYPZl8m077rVTBKtaHmh30OoSfc0eCKOcY99AXmGOMyQB8f4BBTAwkJ0Nmpt2RKOWb6g+GY7/Cye12R6Js4o4E8R8gCagIrHCs/nbSDfv1rAYNICvLShJKqUtFDbTudUyE33LHRepJxpgIY8wtxvIn0NUNsXlWTk8mbWZSKn+hkRB+rTYz+TF3XKSuKiJv5izcIyL/wqpN+Dbt6qrU5UUNguO/wYktdkeibOCOJqaPgTRgkON2EvjEDfv1rMhICArSGoRyKxHpKSLbRGSHiDyRz+uPiMhmEdkoIosdTbK+K+pWQHSGVz/ljgTR0BjznGON6V3GmBeABm7Yr2cFBUFUlNYglNuISCDwLnAz0BwYKiLN87ztVyDeGNMKmAm87t0oi6hCXajV2boOoZNb+h13JIizInJtzhMR6QScdcN+Pa9BA00Qyp3aAzscX5TSgRlAH+c3GGOWOuYuA1iFtaKib6s/CE5uhROb7I5EeZk7EsT9wLsikiQiScA7wH1u2K/n5Uz7rZR7RADO3eJSHNsKchfW2KFLiMi9Odf1Dtk963C9ASAB2szkh9zRi2mDMSYWaAW0Msa0Bm4ocWTeEBNjTfl96pTdkSg/IyK3A/FYqydewhgzxRgTb4yJDw8P925weYXUgto3WAnCHc1M6cd03etSwm1LjhpjTjpGVAM84q79elTOtN/azKTcIxWo5/Q80rHtIiLSHXgKa831816KrWSiBsGpHXAssfj7MAZ2TYU5DWFBO8hKd1d0ykM8tSa1eGi/7pXT1XXbNnvjUGXFGqCxiMSISHlgCHDReuoi0hprcGmCMeagDTEWT73+IEHFHzSXthOW3AirRlo1kjMpkPy1e2NUbuepBFE6ujtceSXUrAnDh8Obb+q0G6pEjDGZwIPAAmAL8KVjCvwXRSTB8bY3gErAVyKSKCJzCtidbwmuCXW6F72ZKTsTNr8BP7SEI79Au8lwy+9QuTH88bbn4lVuUewEISJpInIyn1sacIUbY/ScypVh/Xro2hX+9jfo2BESE+2OSpVixpgfjDFNjDENjTHjHdueNcbMcTzuboypbYyJc9wSCt+jD6k/GE4nwdG1rr3/6HpY0B4SH4O6PaD3Fmh8PwQEQpMx1qp1R9Z4NGRVMsVOEMaYysaYKvncKhtjgtwZpEdFRcHcuTBjhjUvU3w8PP64tWa1UuqCyD4QUO7yvZkyz8Cvj1nJ4ew+uHYmXDcLQp06dDUYDkGVYZvWInyZp5qYShcRGDwYtmyxmptefx1atYLFi+2OTCnfUb461LnJmpvJZOf/nv2LreakLW9Ag1HQezNEDbD+xpyVqwINRsKeGdqjyYdpgnBWowZ89BEsWWKd0N27wzPP2B2VUr6j/mA4kwyHV128/fxRWDUKlnQHCYRuS6HDFCupFKTJg5CdATumeDZmVWyaIPLTtSts3GjVJl5+Gb77zu6IlPINkQkQEHxhhldjrFXnvr8Sdn8Ozf8BN2+A2l0uv68qjeGKW2D7ZO3y6qNsSRAuTGh2vYisF5FMEbnVjhipUAH+8x+Ii4MRIyD1ku7sSvmfclXgipthz1dw+k9YngD/GwyhUdBzLcT9E4IquL6/JmPg3H5Inum5mFWxeT1BuDih2R5gBPCFd6PLIzjYunh97hzcfru1wJBS/i5qMJzdC3ObwIEl0OZN6LEKqscWfV91e0DlJrBtkvvjVCVmRw3ClQnNkowxG4ECroR5UdOm8O67sGwZ/POfdkejlP0ieluzvNbuCr02QbOHra6rxSEBVi3iyGo4vNq9caoSsyNBFHVCswJ5bUKzO++EYcPg+efh//0/z5WjVGlQrhL0TYGu86FSTMn3l9PlVQfO+ZxSfZHaaxOaicDkydbUHLfdBkePeq4spUoDceO/jnKVoeEo68K3dnn1KXYkCJcmNPM5lStb1yP274e77tLFU5RypyYPWtNy7PiP3ZEoJ3YkiMtOaOaz4uPh1Vdh9myrRqGUco/KjbTLqw/yeoJwZUIzEWknIinAQOA/IvK7t+Ms0LhxcPPN8Mgj1lgJpZR7NB0L5w5YXWiVT7DlGoQLE5qtMcZEGmMqGmNqGmNa2BFnvgICYOpUqF4d+veH336zOyKlyoY6N0KVZvCHdnn1FaX6IrVtatWCb76xVqJr1w7eeUevSShVUiLWtYgjv2iXVx+hCaK4rr7aamLq1g3GjIG//AUOlp71X5TySTF3WqO1deCcT9AEURK1alnzNE2aBIsWWTPALlhgd1RKlV7lKluzwO75Es7stTsav6cJoqRErBrEmjUQHg49e1oXsM+XjqWGlfI5TR4Ak6VdXn2AJgh3adkSfvkFHnwQ/v1v6NABvv8esu2fLUSpUqVyI7iiF+x4H7L0i5adNEG4U4UK8PbbVrPT0aPQuze0aAEffGBN+KeUck3TsXDu4IVpxZUtNEF4Qq9esHMnTJsGoaFw773W0qYvvACenDNKqbKiTneocqV1sVp7CNpGE4SnlCtnzdu0di0sXWo1OT3/vJUo7rvPmvQvI8PuKJXyTSLQdAwcXWvN9KpsoQnC00SgSxeYOxc2b4Y77oBPP4XrrrOWOO3dGyZOhE2b9JuSUs6i74ByVbXLq400QXjTlVfClCnWhH/ffGMliz/+gIcfti5yX3GFtTDRjz/aHalS9itXydHl9Svt8moTTRB2qFYN+vWD996zEkRSEnz0kbUW9o8/wk03Wc1R2gNK+bvcLq/v2x0JnD8Cx3+DbP9ZWTLI7gAUUL8+jBpl3c6dg9GjrQvaGzfCZ59BpUp2R6iUPSo3tFaw2/4+tHgKAoO9W/7pPZDyLaTMgoMrrGRVvgbUvgHq3mhdTK/UwLsxeZEmCF8TEgIff2yNyv773+Gaa+Dbb63FipTyR03HQupc+PO/0OBOz5ZlDJzcAsmzrKRwdJ21vWpzaP6ENUbj4HLYtxCSZ1qvVWpgJYo63a3EEVzTszF6kSYIXyRiXZdo0QIGD7YmBJw507rYrZS/qd3N6vL6xySIucP6+3Ankw1H1lgJIXkWpP1hba/ZEeJehch+UKXJhfc3GOFIJNtg/yLYvxCSpsOOKYBAjTbWzLR1ukN4JwgMcW+8XqQJwpf16GGNzk5IgBtvtOZ8Gj3a7qhUIUSkJ/AWEAh8aIx5Nc/r1wMTgVbAEGPMTK8HWdqIWLWINaPh8M8Qfk3J95mdAQeWWUkh5Vs4uxckCGp3hWbjIKIPhF5ReExVm1m3po7V8I6ssZLF/kWwZQJsftVKDuHXOWoXXaF8dUAcSU6K+DgAAgJBAq1YJdDp5uak6aAJwtc1bgyrVsGwYfDXv8L//mfNIBsVZd0iI60R3Mp2IhIIvAvcCKQAa0RkjjFms9Pb9gAjgL97P8JSLOYOSHzC6vJanARhsiFth9VktPcHSP0OMo5DYChc0dOqJUT0cvwDL4aAIAi/2rq1fBYyTllNUfsXWbfEx4u3X5eJFcNFSSPP85sTISSsSHvVBFEaVK1qXYd45hl4/XVrhLaz8HArWdSrB2FhVi+pnFv16hceV61qra1dpYp1Hxjo9R+ljGsP7DDG7AIQkRlAHyA3QRhjkhyvaRe1ogiqCA3vhm1vwZlUCI0o+L1Z6XDidzj2q9NtA2Sesl4vXwPq9bWSQp0bIcgDX7DKVbISTkQv6/nZfXDof5B11jHeyXErymOTbV0kz3vLzsx/u8mzvRgX+DVBlBaBgfDPf8Jzz0FqKuzZc+GWnGzdb98Oq1fD8eNw9uzl9xkaeiFZ1KgBERFWjSTvfe3a1uy0p07B6dMF3+d3K18e6tSBunWte+fHlStfqBpnZ1u3rCzr3hirZuShqrOHRADJTs9TgA7F2ZGI3AvcCxAVFVXyyMqCJg/A1jetHk2xL1nbMk7B8Q1wdP2FZHDid6sJCSCoElSPta4bVG9t3aq1tL5te1OFuhB1q3fLdANNEKVNcDA0aGDdCnPuHJw4YSWLY8esW1oanDyZ//3hw7BlCyxcaD0vroAAqFjRuoWGQnq6NTAwM/PS9wYGWomgoPEeAQH514SqVYOgIGuf+d3ASi753YKDrSSUmWlNdeL8uYwMa+Gnbt2K//O7iTFmCjAFID4+XofYA1SKgYi/wI7JkLbdSgZp27G+aQPB4VYCaHbThWRQuRGIDvcqLk0QZVVIiHWrXbvonz150qqlpKRY9wcPWvuqWNEak5Fzn/PY+RYcfOm3/uxsK0Ht32/d9u2z7o8etZJAYOCl92AlquPHL75t2WLdZ2VZSSK/mzFWDSrvLb+pTAICLv5sdHRJEkQqUM/peaRjm3KXKx+Fxd9b8zNVbw3Rt0MNRzKocEVpq3H6PE0Q6lJVqli3K690z/4CAqBmTevWooV79llUxli1mfPnL04IAW79drkGaCwiMViJYQhwmzsL8Hu1roXB563ePMrjtO6l/IOIVbupUsVq+ipf3t3JAWNMJvAgsADYAnxpjPldRF4UkQQrDGknIinAQOA/IvK7W4PwB5ocvEZrEEq5kTHmB+CHPNuedXq8BqvpSSmfpzUIpZRS+RJTRtYgEJFDwJ8FvBwGHPZiOIXRWC7lK3FA4bHUN8aEezMY8Olz266y/fFn9mTZBZ7XZSZBFEZE1hpj4u2OAzQWX44DfCsWV9gZr11l++PPbFfZ2sSklFIqX5oglFJK5ctfEsQUuwNworFcylfiAN+KxRV2xmtX2f74M9tStl9cg1BKKVV0/lKDUEopVUSaIJRSSuWrzCcIEekpIttEZIeIPGFzLEki8puIJIrIWi+W+7GIHBSRTU7baojIQhHZ7rgv5kopbonleRFJdRyXRBG5xQtx1BORpSKyWUR+F5GHHNttOS5FZdd5XdBx8yYRCRSRX0XkOy+XW01EZorIVhHZIiJXe6nchx3HepOITBcRr61hWqYThNMKXzcDzYGhItLc3qjoaoyJ83J/5qlAzzzbngAWG2MaA4sdz+2KBeDfjuMS55iuwtMygb8ZY5oDHYEHHOeGXcfFZTaf1wUdN296CGuuK297C5hvjGkGxHojBhGJAMYC8caYq7CWsh3i6XJzlOkEgdMKX8aYdCBnhS+/YoxZARzNs7kP8Knj8adAXxtj8TpjzD5jzHrH4zSsP/YIbDouRWTbeV3IcfMKEYkEegEfeqtMR7lVgeuBjwCMMenGmONeKj4IqCAiQUAosNdL5Zb5BJHfCl9eO5nzYYAfRWSdY8UwO9U2xuxzPN4PFGPhCLd6UEQ2OpqgvNqsIyLRQGtgNb53XPLjE+d1nuPmLROBxwBvL9kaAxwCPnE0b30oIhU9XagxJhWYgLWW+T7ghDHmR0+Xm6OsJwhfc60xpg1W08ADInK93QEBGJO7AK5dJgMNgTisP4J/eatgEakEfA2MM8acdH7NB46LzyrsuHmwzN7AQWPMOm+Ul0cQ0AaYbIxpDZzGC82Pji9LfbAS1BVARRG53dPl5ijrCcKnVvhyfBvAGHMQmIXVVGCXAyJSF8Bxf9CuQIwxB4wxWcaYbOADvHRcRKQc1j+5acaYbxybfea4FMLW87qA4+YNnYAEEUnCala7QUT+z0tlpwApxpic2tJMrIThad2B3caYQ8aYDOAb4BovlAuU/QSRu8KXiJTHurgzx45ARKSiiFTOeQz0ADYV/imPmgMMdzweDnxrVyA5/5Ad+uGF4yIigtWevMUY86bTSz5zXAph23ldyHHzOGPMP4wxkcaYaKyfeYkxxivfpo0x+4FkEWnq2NQN2OyFovcAHUUk1HHsu+HFC/RlesEgY0ymiOSs8BUIfGyMsWsFr9rALOt3TBDwhTFmvjcKFpHpQBcgzLGa2XPAq8CXInIX1lTSg2yMpYuIxGE15yQB93khlE7AHcBvIpLo2PYkNh2XorD5vM73uHmp55ndxgDTHEl5FzDS0wUaY1aLyExgPVYPsl/x4pQbOtWGUkqpfJX1JiallFLFpAlCKaVUvjRBKKWUypcmCKWUUvnSBKGUUipfmiBKKRHJcpr9NNGdM3qKSLTzbKtKeYue176lTI+DKOPOGmPi7A5CKTfT89qHaA2ijBFrzYnXxVp34hcRaeTYHi0iSxwT4i0WkSjH9toiMktENjhuOcP4A0XkA8c89D+KSAXH+8c61gLYKCIzbPoxlZ/R89oemiBKrwp5quKDnV47YYxpCbyDNfslwNvAp8aYVsA0YJJj+yRguTEmFmtumZwRuY2Bd40xLYDjwADH9ieA1o793O+ZH035MT2vfYiOpC6lROSUMaZSPtuTgBuMMbsck6rtN8bUFJHDQF1jTIZj+z5jTJiIHAIijTHnnfYRDSx0LJqDiDwOlDPGvCwi84FTwGxgtjHmlId/VOVH9Lz2LVqDKJtMAY+L4rzT4ywuXK/qhbWaWRtgjViLmCjlDXpee5kmiLJpsNP9z47HK7mwVOEw4CfH48XAaMhd67dqQTsVkQCgnjFmKfA4UBW45NueUh6i57WXaZYsvSo4zaYJ1lq5OV0Cq4vIRqxvS0Md28ZgrYb1KNbKWDkzUT4ETHHMXpqF9Ue1j/wFAv/n+GMTYJIXl11U/kHPax+i1yDKGEdbbbwx5rDdsSjlLnpe20ObmJRSSuVLaxBKKaXypTUIpZRS+dIEoZRSKl+aIJRSSuVLE4RSSql8aYJQSimVr/8PFmWGe3/KDawAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_path = os.path.join(os.getcwd(), \"metrics1.pkl\")\n",
    "download_file_from_google_drive(\"1bfeE9c6u8Q299BFVuawrs5Xd8Ioa8wQu\", metrics_path)\n",
    "with open(metrics_path, 'rb') as metrics_file:\n",
    "    metrics = pickle.load(metrics_file)\n",
    "    # plot the graphs\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    # plot train acc and train loss\n",
    "    epochs = np.linspace(0, 20, 20)\n",
    "    axs[0,0].plot(epochs, metrics['train_acc'],color = 'blue', label=\"Train Accuracy\")\n",
    "    axs[0,0].set(ylabel='Accuracy')\n",
    "    axs[0, 0].legend()\n",
    "    axs[1,0].plot(epochs, metrics['train_loss'],color = 'red', label=\"Train Loss\")\n",
    "    axs[1,0].set(ylabel='Loss')\n",
    "    axs[1, 0].legend()\n",
    "    axs[0,1].plot(range(0,10,1), metrics['val_acc'], color = 'green', label=\"Validation Accuracy\")\n",
    "    axs[1,0].set(xlabel='Epochs')\n",
    "    axs[0, 1].legend()\n",
    "    axs[1,1].plot(range(0,10,1), metrics['val_loss'], color = 'orange', label=\"Validation Loss\")\n",
    "    axs[1,1].set(xlabel='Epochs')\n",
    "    axs[1, 1].legend()\n",
    "    fig.savefig('plot.png')\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdca402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
